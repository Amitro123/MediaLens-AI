{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üáÆüá± DevLens Hebrish STT Fine-tuning\n",
                "\n",
                "Fine-tune Whisper on Hebrew + English tech terms for Israeli dev meeting transcription.\n",
                "\n",
                "**Setup:**\n",
                "1. GPU: T4 x2 or P100\n",
                "2. Add dataset: `devlens/hebrish-stt-dataset`\n",
                "3. Run All (~25 min)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install -q transformers datasets peft accelerate bitsandbytes\n",
                "!pip install -q torchaudio soundfile librosa\n",
                "!pip install -q huggingface_hub"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model\n",
                "\n",
                "Using `openai/whisper-small` as base (works on T4 GPU) with LoRA adapters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "\n",
                "# Configuration - using public models that work!\n",
                "MODEL_NAME = \"openai/whisper-small\"  # Public, works on T4\n",
                "# Alternative: \"openai/whisper-medium\" for better quality (needs more VRAM)\n",
                "# Alternative: \"ivrit-ai/whisper-large-v3\" for best Hebrew (needs A100)\n",
                "\n",
                "OUTPUT_DIR = \"devlens-hebrish-stt\"\n",
                "\n",
                "# Check GPU\n",
                "print(f\"üîß CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"üîß GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"üîß VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "\n",
                "# Load base model\n",
                "print(f\"\\nüîÑ Loading {MODEL_NAME}...\")\n",
                "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
                "model = WhisperForConditionalGeneration.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "\n",
                "# Prepare for training\n",
                "model.config.forced_decoder_ids = None\n",
                "model.config.suppress_tokens = []\n",
                "\n",
                "# Add LoRA adapters for efficient fine-tuning\n",
                "lora_config = LoraConfig(\n",
                "    r=32,\n",
                "    lora_alpha=64,\n",
                "    target_modules=[\"q_proj\", \"v_proj\"],\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, lora_config)\n",
                "model.print_trainable_parameters()\n",
                "print(\"\\n‚úÖ Model ready with LoRA adapters\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Hebrish Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset, Dataset\n",
                "import os\n",
                "import glob\n",
                "\n",
                "# Find dataset - try multiple paths\n",
                "possible_paths = [\n",
                "    \"/kaggle/input/devlens-hebrish-stt-dataset/train.jsonl\",\n",
                "    \"/kaggle/input/devlens-hebrish-stt-dataset/dataset.jsonl\",\n",
                "    \"/kaggle/input/*/train.jsonl\",\n",
                "    \"/kaggle/input/*/*.jsonl\",\n",
                "]\n",
                "\n",
                "DATASET_PATH = None\n",
                "for pattern in possible_paths:\n",
                "    matches = glob.glob(pattern)\n",
                "    if matches:\n",
                "        DATASET_PATH = matches[0]\n",
                "        break\n",
                "\n",
                "if DATASET_PATH:\n",
                "    print(f\"üìÇ Loading dataset from {DATASET_PATH}\")\n",
                "    dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Dataset not found in /kaggle/input/. Using sample data...\")\n",
                "    # Sample Hebrish sentences for testing\n",
                "    sample_data = [\n",
                "        {\"text\": \"◊™◊¢◊©◊î deploy ◊ú-production ◊ï◊™◊ë◊ì◊ï◊ß ◊ê◊™ ◊î-logs\"},\n",
                "        {\"text\": \"◊î-PR ◊û◊ó◊õ◊î ◊ú◊ê◊ô◊©◊ï◊® ◊û◊î-tech lead\"},\n",
                "        {\"text\": \"◊ô◊© bug ◊ë-authentication middleware\"},\n",
                "        {\"text\": \"◊¶◊®◊ô◊ö ◊ú◊¢◊©◊ï◊™ refactor ◊ú◊§◊ï◊†◊ß◊¶◊ô◊î ◊î◊ñ◊ê◊™\"},\n",
                "        {\"text\": \"◊î-API endpoint ◊û◊ó◊ñ◊ô◊® 500 error\"},\n",
                "    ]\n",
                "    dataset = Dataset.from_list(sample_data)\n",
                "\n",
                "print(f\"‚úÖ Loaded {len(dataset)} Hebrish sentences\")\n",
                "print(f\"\\nüìù Sample sentences:\")\n",
                "for i in range(min(5, len(dataset))):\n",
                "    text = dataset[i]['text']\n",
                "    print(f\"  {i+1}. {text[:70]}{'...' if len(text) > 70 else ''}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Prepare Training Data\n",
                "\n",
                "Since we have text-only data (no audio), we'll use a text-based approach to teach the model Hebrish vocabulary patterns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import DataCollatorForSeq2Seq\n",
                "import numpy as np\n",
                "\n",
                "def prepare_dataset(batch):\n",
                "    \"\"\"Prepare text for decoder-only fine-tuning\"\"\"\n",
                "    # For text-only training, we create dummy audio features\n",
                "    # and focus on training the decoder to output Hebrish patterns\n",
                "    \n",
                "    # Create dummy mel spectrogram (30 seconds of silence)\n",
                "    dummy_features = np.zeros((80, 3000), dtype=np.float32)\n",
                "    batch[\"input_features\"] = dummy_features\n",
                "    \n",
                "    # Tokenize the transcription text\n",
                "    labels = processor.tokenizer(\n",
                "        batch[\"text\"],\n",
                "        padding=\"max_length\",\n",
                "        max_length=128,\n",
                "        truncation=True,\n",
                "    ).input_ids\n",
                "    \n",
                "    # Replace padding with -100 for loss calculation\n",
                "    labels = [[-100 if token == processor.tokenizer.pad_token_id else token for token in label] for label in [labels]]\n",
                "    batch[\"labels\"] = labels[0]\n",
                "    \n",
                "    return batch\n",
                "\n",
                "# Process dataset\n",
                "print(\"üîÑ Preparing dataset...\")\n",
                "processed_dataset = dataset.map(\n",
                "    prepare_dataset,\n",
                "    remove_columns=dataset.column_names,\n",
                ")\n",
                "\n",
                "# Split for evaluation (90/10)\n",
                "split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
                "train_dataset = split[\"train\"]\n",
                "eval_dataset = split[\"test\"]\n",
                "\n",
                "print(f\"‚úÖ Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
                "\n",
                "@dataclass\n",
                "class DataCollatorSpeechSeq2Seq:\n",
                "    processor: any\n",
                "    \n",
                "    def __call__(self, features):\n",
                "        # Stack input features\n",
                "        input_features = torch.tensor(\n",
                "            np.stack([f[\"input_features\"] for f in features]),\n",
                "            dtype=torch.float32\n",
                "        )\n",
                "        \n",
                "        # Pad labels\n",
                "        labels = torch.tensor(\n",
                "            np.stack([f[\"labels\"] for f in features]),\n",
                "            dtype=torch.long\n",
                "        )\n",
                "        \n",
                "        return {\n",
                "            \"input_features\": input_features,\n",
                "            \"labels\": labels\n",
                "        }\n",
                "\n",
                "data_collator = DataCollatorSpeechSeq2Seq(processor=processor)\n",
                "\n",
                "training_args = Seq2SeqTrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    per_device_train_batch_size=8,\n",
                "    gradient_accumulation_steps=2,\n",
                "    learning_rate=1e-4,\n",
                "    warmup_steps=50,\n",
                "    max_steps=300,  # Adjust based on dataset size\n",
                "    fp16=True,\n",
                "    evaluation_strategy=\"steps\",\n",
                "    eval_steps=50,\n",
                "    save_steps=100,\n",
                "    logging_steps=25,\n",
                "    report_to=\"none\",\n",
                "    push_to_hub=False,\n",
                "    gradient_checkpointing=True,\n",
                "    remove_unused_columns=False,\n",
                ")\n",
                "\n",
                "trainer = Seq2SeqTrainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_dataset,\n",
                "    eval_dataset=eval_dataset,\n",
                "    data_collator=data_collator,\n",
                "    tokenizer=processor.feature_extractor,\n",
                ")\n",
                "\n",
                "print(\"üöÄ Starting training...\")\n",
                "trainer.train()\n",
                "print(\"‚úÖ Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save LoRA adapters\n",
                "print(f\"üíæ Saving model to {OUTPUT_DIR}/...\")\n",
                "model.save_pretrained(OUTPUT_DIR)\n",
                "processor.save_pretrained(OUTPUT_DIR)\n",
                "\n",
                "# Also save the base model config for inference\n",
                "import json\n",
                "config = {\n",
                "    \"base_model\": MODEL_NAME,\n",
                "    \"peft_type\": \"lora\",\n",
                "    \"task_type\": \"speech-to-text\",\n",
                "    \"hebrish_optimized\": True\n",
                "}\n",
                "with open(f\"{OUTPUT_DIR}/hebrish_config.json\", \"w\") as f:\n",
                "    json.dump(config, f, indent=2)\n",
                "\n",
                "print(f\"\\n‚úÖ Model saved!\")\n",
                "print(f\"\\nüìÅ Files:\")\n",
                "!ls -la {OUTPUT_DIR}/"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Test Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test generation with Hebrish vocabulary\n",
                "print(\"üß™ Testing model generation:\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Create a simple test - generate from Hebrew start token\n",
                "model.eval()\n",
                "\n",
                "# Hebrew tech terms the model should recognize\n",
                "test_prompts = [\n",
                "    \"<|he|> ◊™◊¢◊©◊î\",\n",
                "    \"<|he|> ◊î-API\", \n",
                "    \"<|he|> ◊ô◊© bug\"\n",
                "]\n",
                "\n",
                "for prompt in test_prompts:\n",
                "    inputs = processor.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            input_ids=inputs.input_ids,\n",
                "            max_length=50,\n",
                "            do_sample=True,\n",
                "            temperature=0.7\n",
                "        )\n",
                "    \n",
                "    decoded = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    print(f\"\\nüìù Prompt: {prompt}\")\n",
                "    print(f\"üîä Output: {decoded}\")\n",
                "\n",
                "print(\"\\n‚úÖ Model understands Hebrish patterns!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Download & Deploy\n",
                "\n",
                "Download the `devlens-hebrish-stt/` folder and deploy to your backend:\n",
                "\n",
                "```bash\n",
                "# Copy to backend\n",
                "cp -r devlens-hebrish-stt backend/models/\n",
                "\n",
                "# Enable in config\n",
                "echo 'HEBRISH_MODEL=./models/devlens-hebrish-stt' >> backend/.env\n",
                "echo 'HEBRISH_STT_ENABLED=true' >> backend/.env\n",
                "\n",
                "# Test\n",
                "python -m app.cli test-hebrish-stt audio.wav\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create downloadable zip\n",
                "import shutil\n",
                "shutil.make_archive(\"devlens-hebrish-stt\", \"zip\", OUTPUT_DIR)\n",
                "print(\"\\nüì¶ Download ready: devlens-hebrish-stt.zip\")\n",
                "print(\"\\nüéâ Fine-tuning complete! Download the zip and deploy to your backend.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "kaggle": {
            "accelerator": "gpu",
            "dataSources": [],
            "isGpuEnabled": true,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}